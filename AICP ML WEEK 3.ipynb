{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Null Values:\n",
      "Top queries    0\n",
      "Clicks         0\n",
      "Impressions    0\n",
      "CTR            0\n",
      "Position       0\n",
      "dtype: int64\n",
      "\n",
      "Column Information:\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 1000 entries, 0 to 999\n",
      "Data columns (total 5 columns):\n",
      " #   Column       Non-Null Count  Dtype  \n",
      "---  ------       --------------  -----  \n",
      " 0   Top queries  1000 non-null   object \n",
      " 1   Clicks       1000 non-null   int64  \n",
      " 2   Impressions  1000 non-null   int64  \n",
      " 3   CTR          1000 non-null   object \n",
      " 4   Position     1000 non-null   float64\n",
      "dtypes: float64(1), int64(2), object(2)\n",
      "memory usage: 39.2+ KB\n",
      "None\n",
      "\n",
      "Descriptive Statistics:\n",
      "          Clicks   Impressions     Position\n",
      "count  1000.0000   1000.000000  1000.000000\n",
      "mean    172.2750   1939.466000     3.985930\n",
      "std     281.0221   4856.702605     2.841842\n",
      "min      48.0000     62.000000     1.000000\n",
      "25%      64.0000    311.000000     2.010000\n",
      "50%      94.0000    590.500000     3.120000\n",
      "75%     169.0000   1582.750000     5.342500\n",
      "max    5223.0000  73380.000000    28.520000\n"
     ]
    }
   ],
   "source": [
    "# Anomaly Detection in Queries\n",
    "\n",
    "# // Question No. 1 \\\\\n",
    "\n",
    "# Import necessary libraries\n",
    "import pandas as pd\n",
    "\n",
    "# Load the dataset\n",
    "df = pd.read_csv(\"E:\\MNS-UET BS IT\\AICP ML INTERNSHIP\\ML Internship Task 3/Queries.csv\")\n",
    "\n",
    "# Check for null values\n",
    "print(\"Null Values:\")\n",
    "print(df.isnull().sum())\n",
    "\n",
    "# Check column information\n",
    "print(\"\\nColumn Information:\")\n",
    "print(df.info())\n",
    "\n",
    "# Descriptive statistics of the data\n",
    "print(\"\\nDescriptive Statistics:\")\n",
    "print(df.describe())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# // Question No. 2 \\\\\n",
    "\n",
    "# Convert the CTR column from percentage string to float\n",
    "df['CTR'] = df['CTR'].str.rstrip('%').astype(float) / 100\n",
    "\n",
    "# Display the updated DataFrame\n",
    "print(df.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# // Question No. 3 \\\\\n",
    "\n",
    "# Import necessary libraries\n",
    "import pandas as pd\n",
    "import re\n",
    "from collections import Counter\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Load the dataset\n",
    "df = pd.read_csv(\"E:\\MNS-UET BS IT\\AICP ML INTERNSHIP\\ML Internship Task 3/Queries.csv\")\n",
    "\n",
    "# Define a function to clean and split queries into words\n",
    "def clean_and_split_query(query):\n",
    "    \"\"\"\n",
    "    Clean and split a query into words.\n",
    "    \"\"\"\n",
    "    # Convert to lowercase\n",
    "    query = query.lower()\n",
    "    # Remove special characters and punctuation\n",
    "    query = re.sub(r'[^\\w\\s]', '', query)\n",
    "    # Split the query into words\n",
    "    words = query.split()\n",
    "    return words\n",
    "\n",
    "# Apply the function to clean and split each query\n",
    "df['Cleaned_Query'] = df['Top queries'].apply(clean_and_split_query)\n",
    "\n",
    "# Flatten the list of words\n",
    "all_words = [word for query_words in df['Cleaned_Query'] for word in query_words]\n",
    "\n",
    "# Count the frequency of each word\n",
    "word_freq = Counter(all_words)\n",
    "\n",
    "# Create a DataFrame from the word frequencies\n",
    "word_freq_df = pd.DataFrame(word_freq.items(), columns=['Word', 'Frequency'])\n",
    "\n",
    "# Sort the DataFrame by frequency\n",
    "word_freq_df = word_freq_df.sort_values(by='Frequency', ascending=False)\n",
    "\n",
    "# Plot the word frequencies using seaborn\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.barplot(x='Word', y='Frequency', data=word_freq_df.head(20))\n",
    "plt.title('Top 20 Common Words in Search Queries')\n",
    "plt.xticks(rotation=45, ha='right')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# // Question No. 4 \\\\\n",
    "\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Sort the DataFrame by clicks and impressions\n",
    "top_queries_by_clicks = df.sort_values(by='Clicks', ascending=False).head(10)\n",
    "top_queries_by_impressions = df.sort_values(by='Impressions', ascending=False).head(10)\n",
    "\n",
    "# Plot the top queries by clicks with rotated visualization\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.barplot(x='Top queries', y='Clicks', data=top_queries_by_clicks)\n",
    "plt.title('Top queries by Clicks')\n",
    "plt.xlabel('Query')\n",
    "plt.ylabel('Clicks')\n",
    "plt.xticks(rotation=45, ha='right')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Plot the top queries by impressions with rotated visualization\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.barplot(x='Top queries', y='Impressions', data=top_queries_by_impressions)\n",
    "plt.title('Top queries by Impressions')\n",
    "plt.xlabel('Query')\n",
    "plt.ylabel('Impressions')\n",
    "plt.xticks(rotation=45, ha='right')\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# // Question No. 5 \\\\\n",
    "\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Sort the DataFrame by CTR in descending order to get the queries with the highest CTRs\n",
    "highest_ctr_queries = df.sort_values(by='CTR', ascending=False).head(10)\n",
    "\n",
    "# Sort the DataFrame by CTR in ascending order to get the queries with the lowest CTRs\n",
    "lowest_ctr_queries = df.sort_values(by='CTR', ascending=True).head(10)\n",
    "\n",
    "# Plot the queries with the highest CTRs\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.barplot(x='CTR', y='Top queries', data=highest_ctr_queries, orient='h')\n",
    "plt.title('Top queries with Highest CTRs')\n",
    "plt.xlabel('CTR')\n",
    "plt.ylabel('Query')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Plot the queries with the lowest CTRs\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.barplot(x='CTR', y='Top queries', data=lowest_ctr_queries, orient='h')\n",
    "plt.title('Top queries with Lowest CTRs')\n",
    "plt.xlabel('CTR')\n",
    "plt.ylabel('Query')\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# // Question No. 6 \\\\\n",
    "\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Remove '%' sign from CTR column and convert to float\n",
    "df['CTR'] = df['CTR'].str.rstrip('%').astype(float) / 100.0\n",
    "\n",
    "# Compute the correlation matrix\n",
    "correlation_matrix = df[['Clicks', 'Impressions', 'CTR', 'Position']].corr()\n",
    "\n",
    "# Plot the heatmap\n",
    "plt.figure(figsize=(10, 8))\n",
    "sns.heatmap(correlation_matrix, annot=True, cmap='coolwarm', fmt=\".2f\", annot_kws={\"size\": 12})\n",
    "plt.title('Correlation Matrix')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# // Question No. 7 \\\\ \n",
    "from sklearn.ensemble import IsolationForest\n",
    "\n",
    "# Prepare the data for anomaly detection\n",
    "X = df[['Clicks', 'Impressions', 'CTR', 'Position']]\n",
    "\n",
    "# Train the Isolation Forest model\n",
    "isolation_forest = IsolationForest(contamination=0.1, random_state=42)  # Adjust contamination based on expected anomaly rate\n",
    "isolation_forest.fit(X)\n",
    "\n",
    "# Predict anomalies\n",
    "anomaly_scores = isolation_forest.decision_function(X)\n",
    "anomaly_predictions = isolation_forest.predict(X)\n",
    "\n",
    "# Add anomaly predictions to DataFrame\n",
    "df['Anomaly'] = anomaly_predictions\n",
    "\n",
    "# Visualize anomalies\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.scatter(df['Impressions'], df['Clicks'], c=df['Anomaly'], cmap='coolwarm', alpha=0.5)\n",
    "plt.xlabel('Impressions')\n",
    "plt.ylabel('Clicks')\n",
    "plt.title('Anomalies in Search Queries')\n",
    "plt.colorbar(label='Anomaly')\n",
    "plt.show()\n",
    "\n",
    "# Display queries flagged as anomalies\n",
    "anomalies_df = df[df['Anomaly'] == -1]  # Selecting rows where Anomaly prediction is -1\n",
    "print(\"Anomalies Detected in Search Queries:\")\n",
    "print(anomalies_df[['Top queries', 'Clicks', 'Impressions', 'CTR', 'Position']])\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
